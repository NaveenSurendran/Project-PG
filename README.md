# Speech emotion recognition using deep learning
Emotions are important part of understanding human interactions.Using deep learning  with the help of Ravdess dataset we aim to design an automatic emotion recognition system.
Speech emotion recognition is an important research field in natural language processing and Artificial Intelligence. There are Deep learning techniques such as Convolutional Neural Networks (CNN) have been used to classify speech emotion from audio signals.The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset is one of the most frequently used datasets in emotion recognition research.This dataset includes 24 professional actors (12 female, 12 male) 
Each actor speaks each sentence in two styles with different emotional intensity levels: “calm” and “emotional”.
The RAVDESS dataset includes audio recordings of the actors speaking in seven emotional states: neutral, calm, happy, sad, angry, fearful, and disgust.
CNNs have been used to model the acoustic features of the RAVDESS dataset, including Mel-Frequency Cepstral Coefficients (MFCCs) ,chroma and melspectogram. 
CNNs have also been used to extract higher-level representations from the RAVDESS dataset, such as emotion-related prosodic features.
The performance of the CNN models on the RAVDESS dataset has been shown to be highly accurate, with state-of-the-art results of up to 70.54% accuracy. 
The use of deep learning with the RAVDESS dataset has enabled more accurate speech emotion recognition, which can be used in a variety of applications such as human-computer interaction and automated customer service.
